/* eslint-disable no-console */
/* eslint-disable no-underscore-dangle */

var recTokenizer = require( './recursive-tokenizer.js' );

// This is inspired by `wink-tokenizer`, which is regex driven and
// used recursion. While this still driven by regexes, it does not use recursion.
// The algorithm is outlined below:
// 1. First split on a **single space** character to obtain all the tokens including
//    extra spaces between the tokens (if any). Remember, the extra spaces will
//    appear as empty strings in the array.
// 2. Test each token with `categoryRgxs` and tag its category accordingly. Each regex in
//    this array tests one unique token category, viz. **word**, **number**, or
//    **email**. This array is sorted in the decreasing order of the probability
//    of occurence of token type that it tests â€” this ensures higher
//    execution speed.

var tokenizer = function ( trex, categories, preserve ) {
  // Maximum number of preceding spaces allowed.
  var maxPrecedingSpaces = 65535;
  var processFunctions = [];
  var rgxCatDetectors = trex.ltc;
  var tokenizeRecursively = recTokenizer( categories, preserve );
  // Initialize helper regexes.
  var rgxAnyWithRP = trex.helpers.anyWithRP;
  var rgxAnyWithLP = trex.helpers.anyWithLP;
  var rgxLPanyRP = trex.helpers.LPanyRP;
  var rgxSplitter = trex.helpers.splitter;

  var detectTokenCategory = function ( token ) {
    // console.log( token );
    var cat;
    for ( cat = 0; cat < rgxCatDetectors.length; cat += 1 ) {
      // console.log( token, rgxCatDetectors[ cat ][ 0 ].test( token ),  rgxCatDetectors[ cat ][ 1 ] )
      if ( rgxCatDetectors[ cat ][ 0 ].test( token ) ) return rgxCatDetectors[ cat ][ 1 ];
    }
    return categories.unk;
  }; // detectTokenCategory()


  var processUnk = function ( text, cat, precedingSpaces, doc ) {
    // Match is captured here.
    var match;
    // Splitted non-punctuation portion's category.
    var splitCat;

    // Match with any thing followed by a **right** punctuation.
    match = text.match( rgxAnyWithRP );
    // Non-null indicates that there was a right punctuation in the end.
    if ( match ) {
      // Safely add the text prior to punkt if in cache.
      splitCat = doc._addTokenIfInCache( match[ 1 ], precedingSpaces );
      if ( splitCat === categories.unk ) {
        // Try detecting token category before falling back to recursion.
        splitCat = detectTokenCategory( match[ 1 ] );
        if ( splitCat  === categories.unk ) {
          // Still 'unk', handle it via recursive tokenizer.
          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc );
        } else {
          // Because it is a detected category use `processFunctions()`.
          processFunctions[ splitCat ]( match[ 1 ], splitCat, precedingSpaces, doc );
          doc._addToken( match[ 2 ], categories.punctuation, 0 );
        }
      } else {
        // The split is a added via `addTokenIfInCache()`, simply add the balance.
        doc._addToken( match[ 2 ], categories.punctuation, 0 );
      }
      // All done so,
      return;
    }
    // Match with any thing followed by a **left** punctuation.
    match = text.match( rgxAnyWithLP );
    // Now non-null indicates that there was a left punctuation in the beginning.
    if ( match ) {
      // If match 2 is a valid lexeme, can safley add tokens. Notice insertion
      // sequence has reversed compared to the previous if block.
      if ( doc.isLexeme( match[ 2 ] ) ) {
        doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces );
        doc._addTokenIfInCache( match[ 2 ], 0 );
      } else {
        // Try detecting token category before falling bac k to recursion.
        splitCat = detectTokenCategory( match[ 2 ] );
        if ( splitCat  === categories.unk ) {
          // Still 'unk', handle it via recursive tokenizer.
          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc );
        } else {
          // Because it is a detected category use `processFunctions()`.
          doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces );
          processFunctions[ splitCat ]( match[ 2 ], splitCat, 0, doc );
        }
      }
      // All done so,
      return;
    }
    // Punctuation on both sides!
    match = text.match( rgxLPanyRP );
    if ( match ) {
      // If match 2 is a valid lexeme, can safley add tokens.
      if ( doc.isLexeme( match[ 2 ] ) ) {
        doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces );
        doc._addTokenIfInCache( match[ 2 ], 0 );
        doc._addToken( match[ 3 ], categories.punctuation, 0 );
      } else {
        // Try detecting token category before falling bac k to recursion.
        splitCat = detectTokenCategory( match[ 2 ] );
        if ( splitCat  === categories.unk ) {
          // Still 'unk', handle it via recursive tokenizer.
          tokenizeRecursively( trex.rtc, text, precedingSpaces, doc );
        } else {
          // Because it is a detected category use `processFunctions()`.
          doc._addToken( match[ 1 ], categories.punctuation, precedingSpaces );
          processFunctions[ splitCat ]( match[ 2 ], splitCat, 0, doc );
          doc._addToken( match[ 3 ], categories.punctuation, 0 );
        }
      }
      // All done so,
      return;
    }

    // Nothing worked, treat the whole thing as `unk` and fallback to recursive tokenizer.
    tokenizeRecursively( trex.rtc, text, precedingSpaces, doc );
  }; // processUnk()

  // var processWord = function ( token, cat, precedingSpaces, doc ) {
  //   doc._addToken( token, cat, precedingSpaces );
  // }; // processWord()

  var processWordRP = function ( token, cat, precedingSpaces, doc ) {
    // Handle **special case**, `^[a-z]\.$` will arrive here instead of `shortForm`!
    var tl = token.length;
    if ( tl > 2 ) {
      doc._addToken( token.slice( 0, -1 ), categories.word, precedingSpaces );
      doc._addToken( token.slice( -1 ), categories.punctuation, 0 );
    } else if ( tl === 2 && token[ tl - 1 ] === '.' ) {
        doc._addToken( token, categories.word, precedingSpaces );
      } else {
        doc._addToken( token.slice( 0, -1 ), categories.word, precedingSpaces );
        doc._addToken( token.slice( -1 ), categories.punctuation, 0 );
      }
  }; // processWordRP()

  var processDefault = function ( token, cat, precedingSpaces, doc ) {
    doc._addToken( token, cat, precedingSpaces );
  }; // processDefault()

  var tokenize = function ( doc, text ) {
    // Raw tokens, obtained by splitting them on spaces.
    var rawTokens = [];
    // Contains the number of spaces preceding a token.
    var precedingSpaces = 0;
    // Pointer to the `rawTokens`, whereas `pp` is the previous pointer!
    var p;
    // Token category as detected by the `detectTokenCategory()` function.
    var cat;
    // A temporary token!
    var t;

    rawTokens = text.split( rgxSplitter );

    // Now process each raw token.
    for ( p = 0; p < rawTokens.length; p += 1 ) {
      t = rawTokens[ p ];
      // Skip empty (`''`) token.
      if ( !t ) continue; // eslint-disable-line no-continue
      // Non-empty token:
      if ( t[ 0 ] === ' ' ) {
        // This indicates spaces: count them.
        precedingSpaces = t.length;
        // Cap precedingSpaces to a limit if it exceeds it.
        if ( precedingSpaces > maxPrecedingSpaces ) precedingSpaces = maxPrecedingSpaces;
      } else {
        // A potential token: process it.
        cat = doc._addTokenIfInCache( t, precedingSpaces );
        if ( cat === categories.unk ) {
          cat = detectTokenCategory( t );
          processFunctions[ cat ]( t, cat, precedingSpaces, doc );
        }
        precedingSpaces = 0;
      }
    } // for
  }; // tokenize()

  // Main Code:
  // Specific Processes.
  processFunctions[ categories.unk ] = processUnk;
  processFunctions[ categories.wordRP ] = processWordRP;

  // Default process.
  processFunctions[ categories.emoji ] = processDefault;
  processFunctions[ categories.word ] = processDefault;
  processFunctions[ categories.shortForm ] = processDefault;
  processFunctions[ categories.number ] = processDefault;
  processFunctions[ categories.url ] = processDefault;
  processFunctions[ categories.email ] = processDefault;
  processFunctions[ categories.mention ] = processDefault;
  processFunctions[ categories.hashtag ] = processDefault;
  processFunctions[ categories.emoticon ] = processDefault;
  processFunctions[ categories.time ] = processDefault;
  processFunctions[ categories.ordinal ] = processDefault;
  processFunctions[ categories.currency ] = processDefault;
  processFunctions[ categories.punctuation ] = processDefault;
  processFunctions[ categories.symbol ] = processDefault;
  processFunctions[ categories.tabCRLF ] = processDefault;
  processFunctions[ categories.apos ] = processDefault;
  processFunctions[ categories.alpha ] = processDefault;
  processFunctions[ categories.decade ] = processDefault;

  return tokenize;
}; // tokenizer()

module.exports = tokenizer;


/*
0. value
1. normal
2. lemma
3. tag
4. pos
5. cpos
6. stop
7. sentiment
8. shape
9. prob
10. suffix
11. prefix
*/
