var DocData = require( './dd.js' );
var Doc = require( './doc.js' );
var Cache = require( './cache.js' );
var tokenizer = require( './tokenizer.js' );
var compileTRex = require( './compile-trex.js' );

var constants = require( './constants.js' );

var fsm = require( './automaton.js' );

var search = require( './search.js' );

// Size of a single token.
var tkSize = constants.tkSize;


/**
 * Creates an instance of nlp.
 * @private
 *
 * @param {object} theModel language model.
 * @returns {object} conatining set of API methods for natural language processing.
 * @example
 * const nlp = require( 'wink-nlp' );
 * var myNLP = nlp();
*/
var nlp = function ( theModel ) {

  var methods = Object.create( null );
  // Token Regex; compiled from `model`
  var trex;
  // wink-nlp language `model`.
  var model;
  // Holds instance of `cache` created using the `model`.
  var cache;
  // Configured tokenize.
  var tokenize;
  // Automata
  // 1. NER
  var nerAutomata;
  var nerTransformers;
  // 2. SBD
  var sbdAutomata;
  var sbdTransformers;
  // 3. NEG
  var negAutomata;
  var negSetter;
  // SA
  var saAutomata;

  // Private methods.
  // ## mapRawTokens
  /**
   * Maps the raw tokens to an array of normalized tokens.
   * @private
   *
   * @param {array} tokens from the document data structure.
   * @returns {array} conatining the normalized tokens.
   * @private
  */
  var mapRawTokens = function ( tokens ) {
    // Will contain only the hash of normal of tokenized lexemes.
    var mappedTokens = new Array( tokens.length / tkSize );
    var i, k;
    for ( i = 0; i < tokens.length; i += tkSize ) {
      k = i + 1;
      mappedTokens[ i / tkSize ] = ( tokens[ k ] > 65535 ) ?
                                // Handle contraction's expansion.
                                cache.nox( tokens[ k ] ) :
                                // Handle all other words.
                                cache.normal( tokens[ i ] );
    } // for ( i = 0; i < tokens.length...

    return mappedTokens;
  }; // mapRawTokens()

  // ## load
  /**
   * Loads the model containing the core model along with other applicable
   * models.
   * @private
   *
   * @returns {void} nothing!.
   * @private
  */
  var load = function (  ) {
    // Load language model.
    model = theModel.core();
    // With `intrinsicSize` captured, instantiate cache etc.
    cache = Cache( model, theModel.featureFn ); // eslint-disable-line new-cap
    trex = compileTRex( model.trex );

    // Instantiate tokenizer.
    tokenize = tokenizer( trex, model.tcat.hash, model.preserve );

    // Load & setup SBD model.
    var sbdModel = theModel.sbd();

    sbdAutomata = new Array( sbdModel.machines.length );
    sbdTransformers = new Array( sbdModel.machines.length );
    for ( let i = 0; i < sbdModel.machines.length; i += 1 ) {
      sbdAutomata[ i ] = fsm( cache );
      sbdAutomata[ i ].importJSON( sbdModel.machines[ i ] );
      sbdTransformers[ i ] = sbdModel.transformers[ i ];
    }

    // Load & setup NER model.
    var nerModel = theModel.ner();

    nerAutomata = new Array( nerModel.machines.length );
    nerTransformers = new Array( nerModel.machines.length );
    for ( let i = 0; i < nerModel.machines.length; i += 1 ) {
      nerAutomata[ i ] = fsm( cache );
      nerAutomata[ i ].importJSON( nerModel.machines[ i ] );
      nerTransformers[ i ] = nerModel.transformers[ i ];
    }

    var negModel = theModel.negation();
    negAutomata = fsm( cache );
    negAutomata.importJSON( negModel.machines[ 0 ] );
    negSetter = negModel.setter;

    var saModel = theModel.sa();
    saAutomata = fsm( cache );
    saAutomata.importJSON( saModel.machines[ 0 ] );
  }; // load()

  var computeDocVector = function ( tokens4Automata, dd ) {
    const textTokens = tokens4Automata.map( ( t ) => ( cache.value( t ) ) );
    // Vector containing all 0s i.e. `[ 0, 0, 0,... 0 ]`.
    var zeros = new Array( 100 );
    zeros.fill( 0 );

    // The Document Vector.
    var dv = new Array( 100 );
    // Initialize the document vector to all zeros.
    dv.fill( 0 );

    for ( let k = 0; k < textTokens.length; k += 1 ) {
      // Word vector for kth token.
      const wv = theModel.wordVectors[ textTokens[ k ] ] || zeros;
      const items = k + 1;
      for ( let i = 0; i < 100; i += 1 ) {
        dv[ i ] += ( wv[ i ] - dv[ i ] ) / items;
      }
    }
    var ssq = 0;
    for ( let i = 0; i < 100; i += 1 ) {
      ssq += dv[ i ] * dv[ i ];
    }

    dd.vector = dv;
    dd.l2Norm = Math.sqrt( ssq );
  }; // computeDocVector()

  // Public Methods.
  // ## readDoc
  /**
   * Loads a single document to be processed.
   * @private
   *
   * @param {string} text of the document that you want to process.
   * @returns {object} the document in terms of an object that exposes the API.
   * @example
   * const DOC = "The quick brown fox jumps over the lazy dog";
   * myNLP.readDoc(DOC);
  */
  var readDoc = function ( text ) {
    if ( typeof text !== 'string' ) {
      throw Error( `wink-nlp: expecting a valid Javascript string, instead found "${typeof text}".`);
    }
    // Document Data gets populated here as NLP pipe taks execute!
    var dd = Object.create( null );
    // The `cache` is also part of document data structure.
    dd.cache = cache;
    // Document's tokens; each token is represented as an array of numbers:
    // ```
    // [
    //   hash, // of tokenized lexeme
    //   (nox) + preceding spaces, // expansion's normal
    //   pos + lemma, // pos & lemma are contextual
    //   negation flag // 1 bit at msb
    // ]
    // ```
    dd.tokens = [];
    // Sentences — stored as array of pairs of `[ start, end ]` pointing to the `tokens`.
    dd.sentences = [];
    // Markings are 4-tuples of `start`, `end` **token indexes**,  and `begin & end markers`.
    // The begin & end markers are used to markup the tokens specified.
    dd.markups = [];

    var docData = DocData( dd );  // eslint-disable-line new-cap

    // Start of NLP Pipe
    tokenize( docData, text );

    // Map tokens for automata.
    var tokens4Automata = mapRawTokens( dd.tokens );
    // Sentence Boundary Detection.
    // Set first `Pattern Swap (x)` as `null`.
    var px = null;
    for ( let i = 0; i < sbdAutomata.length; i += 1 ) {
      sbdAutomata[ i ].setPatternSwap( px );
      // For SBD, all tokens are required to extract preceeding spaces.
      px = sbdAutomata[ i ].recognize( tokens4Automata, sbdTransformers[ i ], dd.tokens );
    }
    // Note the `px` is marking **only** the **last token** of the sentence and
    // therefore the start & end token indexes are identical.
    // Map `px` to `dd.sentences`.
    // Start with setting up all tokens placed in to a single sentence. The `si`
    // is **sentence #i**.
    var si = [ 0, ( tokens4Automata.length - 1 ) ];
    // Iterate through the sentences detected.
    for ( let i = 0; i < px.length; i += 1 ) {
      // As **start** will always be correct, only need to update the **end**.
      si[ 1 ] = px[ i ][ 0 ];
      if ( i < ( px.length - 1 ) ) {
        dd.sentences.push( si );
        // Again set the end to the last token, which may get updated if more
        // sentences are found.
        si = [ px[ i ][ 0 ] + 1, ( tokens4Automata.length - 1 ) ];
      }
    }
    // Push the last sentence.
    dd.sentences.push( si );
    // Handle the case when the last sentence is floating i.e. the without a
    // end-of-sentence punctuation sign.
    if ( si[ 1 ] < ( tokens4Automata.length - 1 ) ) {
      dd.sentences.push( [ ( si[ 1 ] + 1 ), ( tokens4Automata.length - 1 ) ] );
    }


    // Compute number of tokens, sentences!
    dd.numOfTokens = dd.tokens.length / tkSize;
    dd.numOfSentences = dd.sentences.length;

    // Named entity detection.
    px = null;
    for ( let i = 0; i < nerAutomata.length; i += 1 ) {
      nerAutomata[ i ].setPatternSwap( px );
      px = nerAutomata[ i ].recognize( tokens4Automata, nerTransformers[ i ] );
    }
    // Entities — storted as array of `[ start, end, entity type ].`
    dd.entities = px;

    // Negation
    px = null;
    px = negAutomata.recognize( tokens4Automata );
    negSetter( px, dd, constants, search );

    // Sentiment Analysis
    px = null;
    px = saAutomata.recognize( tokens4Automata );
    dd.sentiment = 0;
    for ( let i = 0; i < px.length; i += 1 ) {
      px[ i ][ 2 ] = ( +px[ i ][ 2 ] ) / 1000;
      if ( dd.tokens[ ( px[ i ][ 0 ] * tkSize ) + 3 ] > 2147483647 ) {
        // Negative score, subtract it.
        dd.sentiment -= px[ i ][ 2 ];
      } else {
        // Positive score, add it.
        dd.sentiment += px[ i ][ 2 ];
      }
    }
    dd.sentiment /= px.length;
    // console.log( dd.sentiment );

    // Word Vector
    if ( theModel.wordVectors !== undefined ) {
      computeDocVector( tokens4Automata, dd );
    }

    // Now create the document!
    var doc = Doc( dd ); // eslint-disable-line new-cap

    // All done — cleanup document's data.
    docData.clean();
    return doc;
  }; // readDoc()

  // Load the model.
  load( theModel );

  // Methods.
  methods.readDoc = readDoc;

  return methods;
}; // wink

module.exports = nlp;
